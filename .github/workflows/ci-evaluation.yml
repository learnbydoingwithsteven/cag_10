name: CI/CD Evaluation Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly evaluation
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'

jobs:
  # Job 1: Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        app: [
          'app_01_legal_analyzer',
          'app_02_medical_assistant',
          'app_03_code_reviewer',
          'app_04_support_agent',
          'app_05_financial_analyzer',
          'app_06_paper_summarizer',
          'app_07_product_recommender',
          'app_08_educational_tutor',
          'app_09_compliance_checker',
          'app_10_fact_checker'
        ]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r shared/requirements.txt
        pip install -r ${{ matrix.app }}/backend/requirements.txt
    
    - name: Run unit tests
      run: |
        cd ${{ matrix.app }}
        pytest tests/unit/ -v --cov=backend --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./${{ matrix.app }}/coverage.xml
        flags: ${{ matrix.app }}

  # Job 2: Integration Tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
      
      chromadb:
        image: chromadb/chroma:latest
        ports:
          - 8000:8000
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install -r shared/requirements.txt
    
    - name: Pull Ollama models
      run: |
        docker exec ollama ollama pull llama3
        docker exec ollama ollama pull mistral
        docker exec ollama ollama pull codellama
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --maxfail=3
      env:
        OLLAMA_HOST: http://localhost:11434
        CHROMA_HOST: http://localhost:8000
        REDIS_HOST: localhost

  # Job 3: Evaluation Pipeline
  evaluation:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install -r shared/requirements.txt
        pip install -r shared/evaluation/requirements.txt
    
    - name: Download evaluation datasets
      run: |
        python shared/evaluation/download_datasets.py
    
    - name: Run evaluation suite
      run: |
        python shared/evaluation/run_all_evaluations.py \
          --output-dir ./evaluation_results \
          --save-metrics
    
    - name: Generate evaluation report
      run: |
        python shared/evaluation/generate_report.py \
          --input-dir ./evaluation_results \
          --output evaluation_report.html
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-results
        path: |
          ./evaluation_results/
          evaluation_report.html
    
    - name: Post evaluation summary
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = JSON.parse(fs.readFileSync('./evaluation_results/summary.json'));
          
          let comment = '## ðŸ“Š Evaluation Results\n\n';
          comment += '| Metric | Score | Change |\n';
          comment += '|--------|-------|--------|\n';
          
          for (const [metric, data] of Object.entries(summary.metrics)) {
            const change = data.change > 0 ? `+${data.change}%` : `${data.change}%`;
            const emoji = data.change > 0 ? 'ðŸ“ˆ' : data.change < 0 ? 'ðŸ“‰' : 'âž¡ï¸';
            comment += `| ${metric} | ${data.score.toFixed(3)} | ${emoji} ${change} |\n`;
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  # Job 4: Performance Benchmarking
  performance:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install -r shared/requirements.txt
        pip install locust pytest-benchmark
    
    - name: Run performance tests
      run: |
        python shared/evaluation/performance_benchmark.py \
          --duration 300 \
          --users 50 \
          --output performance_results.json
    
    - name: Check performance thresholds
      run: |
        python shared/evaluation/check_thresholds.py \
          --results performance_results.json \
          --thresholds performance_thresholds.yml
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: performance_results.json

  # Job 5: Model Quality Checks
  model-quality:
    runs-on: ubuntu-latest
    needs: evaluation
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install -r shared/requirements.txt
    
    - name: Check response quality
      run: |
        python shared/evaluation/quality_checks.py \
          --check-hallucination \
          --check-bias \
          --check-toxicity \
          --output quality_report.json
    
    - name: Validate citations
      run: |
        python shared/evaluation/validate_citations.py \
          --apps app_01_legal_analyzer app_09_compliance_checker
    
    - name: Upload quality report
      uses: actions/upload-artifact@v3
      with:
        name: quality-report
        path: quality_report.json

  # Job 6: Security Scanning
  security:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run Bandit security scan
      run: |
        pip install bandit
        bandit -r . -f json -o bandit_report.json
    
    - name: Run Safety check
      run: |
        pip install safety
        safety check --json > safety_report.json
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit_report.json
          safety_report.json

  # Job 7: Frontend Tests
  frontend-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        app: [
          'app_01_legal_analyzer',
          'app_02_medical_assistant',
          'app_03_code_reviewer'
        ]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
    
    - name: Install dependencies
      run: |
        cd ${{ matrix.app }}/frontend
        npm ci
    
    - name: Run tests
      run: |
        cd ${{ matrix.app }}/frontend
        npm test -- --coverage --watchAll=false
    
    - name: Build
      run: |
        cd ${{ matrix.app }}/frontend
        npm run build

  # Job 8: E2E Tests
  e2e-tests:
    runs-on: ubuntu-latest
    needs: [frontend-tests, integration-tests]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
    
    - name: Install Playwright
      run: |
        npm install -g playwright
        playwright install
    
    - name: Start services
      run: |
        docker-compose up -d
        sleep 30
    
    - name: Run E2E tests
      run: |
        pytest tests/e2e/ -v --headed
    
    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: e2e-artifacts
        path: tests/e2e/screenshots/

  # Job 9: Deploy to Staging
  deploy-staging:
    runs-on: ubuntu-latest
    needs: [e2e-tests, model-quality, performance]
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add deployment commands here
    
    - name: Run smoke tests
      run: |
        python shared/evaluation/smoke_tests.py \
          --environment staging

  # Job 10: Deploy to Production
  deploy-production:
    runs-on: ubuntu-latest
    needs: [e2e-tests, model-quality, performance]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Add deployment commands here
    
    - name: Run smoke tests
      run: |
        python shared/evaluation/smoke_tests.py \
          --environment production
    
    - name: Notify deployment
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.repos.createDeploymentStatus({
            owner: context.repo.owner,
            repo: context.repo.repo,
            deployment_id: context.payload.deployment.id,
            state: 'success',
            description: 'Deployment successful'
          });
